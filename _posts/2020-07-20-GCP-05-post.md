---
title: Google Cloud Platform - Cloud Dataproc
date : 2020-07-20 11:37:30 -0400
categories : GCP Google Cloud
---

#### 1. Cloud Dataproc 정의 : 클라우드 네이티브 아파치 하둡(Apache Hadoop) 및 아파치 스파크(Apache Spark) 서비스이다.



#### 2. Cloud Dataproc 특징 
* 완전 관리형 클라우드 서비스이기에 더 간단하고 효율적으로 하둡 및 스파크 클레스터를 생성할 수 있다.
* 환경 구축을 위해서 몇시간에서 며칠씩 걸리던 작업이 몇분에서 몇초만에 끝난다.
* 클러스터 배포, 로깅, 모니터링과 같은 관리는 GCP에서 자동으로 지원해주기 때문에 직접 인프라 관리를 할 필요가 없다.
* 언제든 클레스터를 만들고 다양한 가상머신유형, 디스크 크기, 노드 수, 네트워킹 옵션등 여러 리소스를 최적화하고 확장할 수 있다.
* 다수의 마스터 노드 사용해 클러스터를 실행하고 실패해도 다시 시작하도록 설정할 수 있기 때문에 높은 가용성을 가진다.
* 쉬운 Web UI, Cloud SDK, RESTful API등 다양한 방식으로 클러스터를 관리할 수 있다.
* Big Query, 클라우드 스토리지, 클라우드 빅데이터, Stack driver와 같은 다른 구글 서비스들과 기본적으로 통합되기 때문에 단순 스파크 또는 하둡 클러스터 이상의 가치를 얻게 되면서 완벽한 플랫폼을 갖출 수 있다.



#### 3. Apache Hadoop
* 정의 : 분산환경의 병렬처리 프레임워크로, 크게 보면 분산파일시스템인 HDFS(Hadoop Distributed File System) 와 데이터처리를 위한 맵리듀스(MapReduce) 프레임워크로 구성되어 있다.
* 특징 
    + 여러대의 서버를 이용해 하나의 클러스터를 구성하며, 이렇게 클러스터로 묶인 서버의 자원을 하나의 서버처럼 사용할 수 있는 클러스터 컴퓨팅환경을 제공한다.
    + 기본적인 작동 방법은 분석할 데이터를 하둡 파일 시스템인 HDFS에 저장해두고 HDFS상에서 MapReduce 프로그램을 이용해 데이터처리를 수행하는 방식이다.
    + 하나의 네임노드와 여러개의 데이터노드로 구성되며, 하나의 네임노드가 나머지 데이터노드를 관리하는 형태로 작동한다.
* HDFS(Hadoop Distribute File System) : 여러 대의 서버에 데이터를 다중 복제해서 저장하는 방식으로 안정성을 보장하며, 클러스터 자원관리 시스템인 Yarn(MapReduce v2)의 도입과 더불어 하나의 클러스터에 1개이상의 네임노드를 설정할 수 있는 네임노드 페더레이션 기능을 제공하는 등 안정적인 서비스가 가능 해졌다.



#### 4. Apache Spack
* 정의 : 하둡과 유사한 클러스터 기반의 분산기능을 제공하는 오픈소스 프레임워크이다.
* 특징 
    + 처리결과를 항상 파일 시스템에 유지하는 하둡과는 달리, 메모리에 저장하고 관리할 수 있는 인메모리캐싱 기능을 제공한다.
    + 속도가 빠르고 머신러닝 같은 반복적인 데이터처리에 뚸어난 성능을 보인다.
    + 맵리듀스뿐만 아니라 스트리밍(Spack Streaming), 머신러닝(MLib), 그래프처리(Graph X), SQL 처리(Spark SQL)등 범용적인 분산 클러스터 환경을 제공한다.
    + 데이터를 읽고, 변형하고, 집계를 할 수 있으며 복잡한 통계모델들을 쉽게 학습하고 배포할 수 있다.



#### 5. Hadoop Eco System
* 정의 : 하둡과 관련된 프레임워크들이다. 하둡코어프로젝트(HDFS, MapReduce)와 수집,분석등의 하둡 서비 프로젝트로 구성이 되는 데, 다양한 종류의 프레임워크를 제공하기 때문에 사용자의 필요에 따라 다양하게 조합해서 이용할 수 있다.



#### 6. Cloud Dataproc 구성요소
* 설치 : Cloud Dataproc에서 클러스터를 생성하면 표준 아파치 하둡 에코 시스템 구성요소가 자동으로 클러스터에 설치되는데, 클러스터 생성시 '고급 옵션 - 선택적 구성요소'라는 메뉴를 통해서 추가로 구성요소를 설치한다.
* 구성 : Anaconda, Hive WebHCat, Jupyter Notebook, kerberos, Presto, Zeppelin가 있다.
